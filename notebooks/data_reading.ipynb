{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import s3fs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import nltk.corpus\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data reading\n",
    "The raw data are stored in the storage system S3 through SSP cloud."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_id = '5LBXHMTZVFCOYVQRJRMN'\n",
    "access_key = 'z0U21jRxrTyt3NLEtWCgh3euCCSgGCLBCRz9zOyT'\n",
    "token = \"eyJhbGciOiJIUzUxMiIsInR5cCI6IkpXVCJ9.eyJhY2Nlc3NLZXkiOiI1TEJYSE1UWlZGQ09ZVlFSSlJNTiIsImFsbG93ZWQtb3JpZ2lucyI6WyIqIl0sImF1ZCI6WyJtaW5pby1kYXRhbm9kZSIsIm9ueXhpYSIsImFjY291bnQiXSwiYXV0aF90aW1lIjoxNjU1MDYyNTMzLCJhenAiOiJvbnl4aWEiLCJlbWFpbCI6Imlkcmlzc2Eua29ua29ib0BlbnNhZS5mciIsImVtYWlsX3ZlcmlmaWVkIjp0cnVlLCJleHAiOjE2NTUxNDg5MzQsImZhbWlseV9uYW1lIjoiS09OS09CTyIsImdpdmVuX25hbWUiOiJJZHJpc3NhIiwiZ3JvdXBzIjpbXSwiaWF0IjoxNjU1MDYyNTM0LCJpc3MiOiJodHRwczovL2F1dGgubGFiLnNzcGNsb3VkLmZyL2F1dGgvcmVhbG1zL3NzcGNsb3VkIiwianRpIjoiZDQ5ODM2NTMtYjJhMy00NTlmLWJhZDktY2QyOWJlZWZlODJkIiwibG9jYWxlIjoiZW4iLCJuYW1lIjoiSWRyaXNzYSBLT05LT0JPIiwibm9uY2UiOiI2N2UyZjMzNC1lMzQ5LTRlYzAtOWY1Yy04NGVhZTA4ZjZkOWMiLCJwb2xpY3kiOiJzdHNvbmx5IiwicHJlZmVycmVkX3VzZXJuYW1lIjoiaWtvbmtvYm8iLCJyZWFsbV9hY2Nlc3MiOnsicm9sZXMiOlsib2ZmbGluZV9hY2Nlc3MiLCJ1bWFfYXV0aG9yaXphdGlvbiJdfSwicmVzb3VyY2VfYWNjZXNzIjp7ImFjY291bnQiOnsicm9sZXMiOlsibWFuYWdlLWFjY291bnQiLCJtYW5hZ2UtYWNjb3VudC1saW5rcyIsInZpZXctcHJvZmlsZSJdfX0sInNjb3BlIjoib3BlbmlkIHByb2ZpbGUgZ3JvdXBzIGVtYWlsIiwic2Vzc2lvbl9zdGF0ZSI6ImE1MGYxZGQ1LThhNWItNGEzZS1iODg5LWM2MmQ5ZjNjMDM5OCIsInNpZCI6ImE1MGYxZGQ1LThhNWItNGEzZS1iODg5LWM2MmQ5ZjNjMDM5OCIsInN1YiI6IjBiMWM2YWM4LTcyZTQtNDkxYy1iMmYwLTIwOGZmYjhiMWNiNSIsInR5cCI6IkJlYXJlciJ9.e87ZqCZne_FpGavDqgH6D6jwJ8oCWBhvKX6TQ6-vQjnFVFpnAJZBBMFEj8BsjL5wLEr6PhPhxTl_y-vZ3-qCtQ\"\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://'+'minio.lab.sspcloud.fr'}, key = key_id, secret = access_key, token = token)\n",
    "\n",
    "comms_df = pd.read_sas(fs.open(\"ikonkobo/commsdata/commsdata.sas7bdat\"), format='sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        b'Customer service is no longer an option with...\n",
       "1        b'My phone was stollen 5 days ago  I got the n...\n",
       "2        b'MTT needs to lower their plans    Im conside...\n",
       "3        b'Phones were turned off due to past due bill ...\n",
       "4                                          b'It was great'\n",
       "                               ...                        \n",
       "56552    b'Great customer service and very helpful  Tha...\n",
       "56553    b'Acqueline fantastic great personality answer...\n",
       "56554     b'americans serving americans always works best'\n",
       "56555       b'Wish we had LTE or faster 4g in Olympia  WA'\n",
       "56556    b'It was very frustrating when I was disconnec...\n",
       "Name: verbatims, Length: 56557, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comms_df[\"verbatims\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = comms_df[\"verbatims\"].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[\"verbatims\"] = df_[\"verbatims\"].apply(lambda x : x.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_[\"verbatims_process\"] = df_[\"verbatims\"].apply(lambda x : re.sub(\"[\\,.?!]\", \"\", x))\n",
    "df_[\"verbatims_process\"] = df_[\"verbatims_process\"].apply(lambda x : x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        # deacc=True removes punctuations\n",
    "        yield(simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) \n",
    "             if word not in stop_words] for doc in texts]\n",
    "data = df_.verbatims_process.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "# remove stop words\n",
    "data_words = remove_stopwords(data_words)\n",
    "#print(data_words[:1][0][:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_words)\n",
    "# Create Corpus\n",
    "texts = data_words\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "# View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# number of topics\n",
    "num_topics = 15\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=id2word,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "# pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lda_model.get_topic_terms(5))\n",
    "topic_word = [id2word[term[0]] for term in lda_model.get_topic_terms(5)]     \n",
    "topic = '\"'+','.join(topic_word)+'\"'\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic(ldamodel, topic):\n",
    "    topic_word = [id2word[term[0]] for term in ldamodel.get_topic_terms(topic)]     \n",
    "    return '\"'+','.join(topic_word)+'\"'\n",
    "\n",
    "list_topics = [extract_topic(lda_model, i) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df__ = comms_df.select_dtypes(exclude='float64')\n",
    "#df__\n",
    "df__.drop([\"verbatims\", \"resolution\"], axis=1, inplace=True)\n",
    "comms_df.drop(list(df__.columns), axis=1, inplace=True)\n",
    "df_ = pd.get_dummies(df__, prefix_sep=\"_\", drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df[df_.columns] = df_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(set([word for doc in comms_df[\"verbatims\"].values for word in word_tokenize(doc.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df___ = comms_df[\"verbatims\"].to_frame()\n",
    "df___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for word in words:\n",
    "        df___[word] = df___[\"verbatims\"].apply(lambda x: x.count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df[\"verbatims\"].values[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df__.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing\n",
    "\n",
    "The project focuses on churn prediction and the dataset has 2 potential target variables which are \n",
    "+ *churn* : indicates wheteher customer churned \n",
    "+ *upsell_xsell* : indicates customerâ€™s flag for cross-sell or up-sell. \n",
    "Since we focus on churn prediction, *upsell_xsell* will be deleted. \n",
    "\n",
    "Furthermore, the variables listed below are useless for predictive modeling and will be rejected :\n",
    "+ city\n",
    "+ city_lat\n",
    "+ city_long\n",
    "+ data_usage_amt\n",
    "+ mou_onnet_6m_normal\n",
    "+ mou_roam_6m_normal\n",
    "+ region_lat\n",
    "+ region_long\n",
    "+ state_lat\n",
    "+ state_long\n",
    "+ tweedie_adjusted\n",
    "\n",
    "In addition, we notice that character variables are showed in this format b'prime'. We will suppress characters 'b' and '."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.drop([\"upsell_xsell\", \"city\", \"city_lat\", \"city_long\", \"data_usage_amt\", \"mou_onnet_6m_normal\", \"mou_roam_6m_normal\", \"region_lat\",\n",
    "\"region_long\", \"state_lat\", \"state_long\", \"tweedie_adjusted\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_vars_object = list(comms_df.select_dtypes(exclude = ['int64', 'float64']).columns)\n",
    "\n",
    "for var in list_vars_object:\n",
    "    comms_df[var] = comms_df[var].apply(lambda x : x.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_mqtes = comms_df.isnull().sum().to_frame().reset_index()\n",
    "df_val_mqtes.columns = [\"variable\", \"nb_valeur_manquante\"]\n",
    "df_val_mqtes = df_val_mqtes[df_val_mqtes.nb_valeur_manquante > 0].reset_index(drop=True)\n",
    "df_val_mqtes['pourcent_valeur_manquante'] = round(100 * df_val_mqtes['nb_valeur_manquante'] / comms_df.shape[0], 2)\n",
    "df_val_mqtes = df_val_mqtes.sort_values('nb_valeur_manquante', ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(df_val_mqtes.variable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_ = [var for var in list(df_val_mqtes.variable) if len(comms_df[var].unique()) < 50]\n",
    "\n",
    "l_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df[\"tot_drpd_pr1\"].value_counts(ascending=False).to_frame().reset_index().iloc[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(comms_df[df_val_mqtes.variable[4]])#, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comms_df.churn.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target variable churn seems to not have missing values. The dataset contains 12.13 % of customers churned. We will split the dataset to 70 % for training and 30% for test using *churn* as startify variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = comms_df.drop(columns=[\"churn\"])\n",
    "y = comms_df.churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = X_train \n",
    "train[\"churn\"] = y_train\n",
    "\n",
    "test = X_test \n",
    "test[\"churn\"] = y_test"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ed5946f2a0d3f5e934efd92075c2a89b2cb5130d0efd6e4509a568bedf48ed26"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('basesspcloud')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
